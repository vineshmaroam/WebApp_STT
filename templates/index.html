<!DOCTYPE html>
<html>
<head>
    <title>Speech-to-Text App</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 1000px; margin: 0 auto; padding: 20px; }
        .header { display: flex; justify-content: space-between; align-items: center; }
        .container { display: flex; gap: 30px; margin-top: 20px; }
        .section { flex: 1; }
        table { width: 100%; border-collapse: collapse; margin-top: 15px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .btn { padding: 8px 12px; cursor: pointer; }
        .btn-danger { background: #dc3545; color: white; border: none; }
        .flashes { background: #ffdddd; border-left: 4px solid #f44336; padding: 10px; margin-bottom: 20px; }
        .user-info { background: #e9f7ef; padding: 10px; border-radius: 5px; }
        .phrase-set-info { background: #e7f4ff; padding: 10px; border-left: 4px solid #2196F3; margin: 10px 0; }
        #recordingStatus { margin: 10px 0; font-weight: bold; }
        #audioVisualizer { width: 100%; height: 50px; background: #f0f0f0; margin: 10px 0; }
        #results { margin-top: 20px; }
        #results div { background: #f5f5f5; padding: 10px; margin: 5px 0; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Welcome, {{ session.username }}!</h1>
        <div class="user-info">
            <a href="/logout" class="btn">Logout</a>
        </div>
    </div>

    {% with messages = get_flashed_messages() %}
        {% if messages %}
            <div class="flashes">
                {% for message in messages %}
                    <p>{{ message }}</p>
                {% endfor %}
            </div>
        {% endif %}
    {% endwith %}

    <div class="phrase-set-info">
        <p>Your phrases are stored in your personal phrase set: <strong>user-{{ session.user_id }}-phrases</strong></p>
    </div>

    <div class="container">
        <div class="section">
            <h2>Upload Audio</h2>
            <form action="/transcribe" method="post" enctype="multipart/form-data">
                <input type="file" name="file" accept=".wav,.mp3,.flac" required>
                <button type="submit" class="btn">Transcribe</button>
            </form>

            <h2>Or Record Audio</h2>
            <div>
                <button id="startRecording" class="btn">Start Recording</button>
                <button id="stopRecording" class="btn" disabled>Stop Recording</button>
                <div id="recordingStatus">Press "Start Recording" to begin</div>
                <div id="audioVisualizer"></div>
            </div>
            <div id="results"></div>
        </div>

        <div class="section">
            <h2>Manage Phrases</h2>
            <form action="/add_phrase" method="post">
                <input type="text" name="phrase" placeholder="Phrase" required>
                <input type="number" name="boost" placeholder="Boost" step="0.1" min="0" value="10" required>
                <button type="submit" class="btn">Add Phrase</button>
            </form>

            <h3>Your Phrases</h3>
            {% if phrases %}
            <table>
                <tr>
                    <th>Phrase</th>
                    <th>Boost</th>
                    <th>Action</th>
                </tr>
                {% for phrase in phrases %}
                <tr>
                    <td>{{ phrase.phrase }}</td>
                    <td>{{ phrase.boost }}</td>
                    <td>
                        <form action="/delete_phrase/{{ phrase.phrase }}" method="post" style="display: inline;">
                            <button type="submit" class="btn btn-danger">Delete</button>
                        </form>
                    </td>
                </tr>
                {% endfor %}
            </table>
            {% else %}
            <p>No phrases added yet</p>
            {% endif %}
        </div>
    </div>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        const startButton = document.getElementById('startRecording');
        const stopButton = document.getElementById('stopRecording');
        const statusDisplay = document.getElementById('recordingStatus');
        const visualizer = document.getElementById('audioVisualizer');
        const resultsDiv = document.getElementById('results');
        let audioContext;
        let analyser;
        let dataArray;
        let animationId;

        // Set up audio context and analyzer when the page loads
        function initAudioContext() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 64;
            const bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);
        }

        // Visualize audio input
        function visualize() {
            if (!analyser) return;

            analyser.getByteFrequencyData(dataArray);

            const width = visualizer.offsetWidth;
            const height = visualizer.offsetHeight;
            const barWidth = width / dataArray.length;
            const canvas = document.createElement('canvas');
            canvas.width = width;
            canvas.height = height;
            const ctx = canvas.getContext('2d');
            visualizer.innerHTML = '';
            visualizer.appendChild(canvas);

            ctx.clearRect(0, 0, width, height);

            for (let i = 0; i < dataArray.length; i++) {
                const barHeight = (dataArray[i] / 255) * height;
                const x = i * barWidth;
                ctx.fillStyle = '#4CAF50';
                ctx.fillRect(x, height - barHeight, barWidth - 2, barHeight);
            }

            animationId = requestAnimationFrame(visualize);
        }

        startButton.addEventListener('click', async () => {
            try {
                if (!audioContext) {
                    initAudioContext();
                }

                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);

                // Connect audio stream to analyzer for visualization
                const source = audioContext.createMediaStreamSource(stream);
                source.connect(analyser);

                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    audioChunks = [];

                    // Send audio to server
                    const formData = new FormData();
                    formData.append('audio_data', audioBlob, 'recording.wav');

                    try {
                        statusDisplay.textContent = 'Processing...';
                        const response = await fetch('/upload_audio_blob', {
                            method: 'POST',
                            body: formData
                        });

                        const result = await response.json();

                        if (result.error) {
                            statusDisplay.textContent = 'Error: ' + result.error;
                        } else {
                            statusDisplay.textContent = 'Recording complete!';
                            resultsDiv.innerHTML = '';

                            if (result.transcripts && result.transcripts.length > 0) {
                                result.transcripts.forEach(transcript => {
                                    const div = document.createElement('div');
                                    div.innerHTML = `<strong>Transcript:</strong> ${transcript.transcript}<br>
                                                    <strong>Confidence:</strong> ${transcript.confidence}`;
                                    resultsDiv.appendChild(div);
                                });
                            } else {
                                resultsDiv.innerHTML = '<div>No speech detected</div>';
                            }
                        }
                    } catch (error) {
                        statusDisplay.textContent = 'Error processing audio';
                        console.error('Error:', error);
                    }

                    cancelAnimationFrame(animationId);
                    visualizer.innerHTML = '';
                };

                mediaRecorder.start();
                startButton.disabled = true;
                stopButton.disabled = false;
                statusDisplay.textContent = 'Recording...';
                visualize();
            } catch (error) {
                console.error('Error:', error);
                statusDisplay.textContent = 'Error accessing microphone: ' + error.message;
            }
        });

        stopButton.addEventListener('click', () => {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                startButton.disabled = false;
                stopButton.disabled = true;
                statusDisplay.textContent = 'Processing recording...';

                // Stop all tracks in the stream
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
        });
    </script>
</body>
</html>
